---

reboot_servers_when_cluster_is_destroyed: true
remove_images_when_cluster_is_destroyed: true
generate_hostname_and_create_etc_hosts: true
add_logrotate_system_messages: true
keep_firewalld_enabled: true
upgrade_packages: false
disable_ipv6: true
sshd_use_ipv4_only: true
firewalld_allowed_ports:
  - "6443/tcp"
  - "10250/tcp"
  - "10251/tcp"
  - "10252/tcp"
  - "10255/tcp"
  - "10053/tcp"
  - "10053/udp"
  - "2379/tcp"
  - "2380/tcp"
  - "22/tcp"
  - "80/tcp"
  - "8080/tcp"
  - "443/tcp"
  - "8443/tcp"
  - "9443/tcp"

firewalld_rich_rules:
  - 'rule family="ipv4" source address="0.0.0.0/0" destination address="0.0.0.0/0" accept'
  - 'rule family="ipv4" source address="0.0.0.0/0" destination address="0.0.0.0/0" masquerade'

modules_to_load:
  - br_netfilter
  - overlay

build_cni_from_source: false
cni_git_url: "https://github.com/containernetworking/plugins"
cni_git_version: "v0.8.1"
cni_build_temp_folder: "~/git"
crio:
  root: "/var/lib/containers/storage"
  runroot: "/var/run/containers/storage"
  config_directory: "/etc/cni/net.d"
  plugins_directory: '[ "/usr/libexec/cni", "/opt/cni/bin/" ]'
  cgroup_manager: cgroupfs
  storage_driver: overlay
  storage_option: '[ "overlay.override_kernel_check=1" ]'
  stream_addr: "0.0.0.0"
  stream_port: "0"
  file_locking: "false"
  apparmor_profile: "crio-default"
  hooks_dir: '[ "/usr/share/containers/oci/hooks.d" ]'
  log_size_max: 52428800
  log_level: info
  log_dir: "/var/log/crio/pods"

cni_crio_config_directory: "/etc/cni/net.d"
crio_plugins_directory: '[ "/usr/libexec/cni", "/opt/cni/bin/" ]'
crio_cgroup_manager: cgroupfs

go_build_required_package:
  - golang
  - golang-bin
  - golang-shared
  - golang-tests
  - golang-misc
  - golang-race

package_list:
  - python3
  - logrotate
  - vim
  - jq
  - wget
  - curl
  - git

# set the master node that performe action or leave black to select it from the masters list using inventory var
master_initiator: 

podman:
  install: true

skopeo:
  install: true

buildah:
  install: true


# flannel, weave, calico, canal
pod_network: calico

pod_network_urls:
  flannel: "https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml"
  weave: "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
  calico: "https://docs.projectcalico.org/v3.10/manifests/calico.yaml"
  canal: "https://docs.projectcalico.org/v3.10/manifests/canal.yaml"

#kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
#kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

# https://github.com/helm/helm/releases
helm:
  install: true
  url: "https://get.helm.sh/helm-v3.0.1-linux-amd64.tar.gz"
  add_repos: true
  update_repos: true
  bin_directory: "/usr/bin"
  repos:
    - name: "incubator"
      url: "https://kubernetes-charts-incubator.storage.googleapis.com/"
    - name: "brigade"
      url: "https://brigadecore.github.io/charts"
    - name: "bitnami"
      url: "https://charts.bitnami.com/bitnami"
    - name: "stable"
      url: "https://kubernetes-charts.storage.googleapis.com"

# metrics_server_install_method accept "git" => {url and plugin_dir become mandatory} or "helm" => {helm_app_name become mandatory}
metrics_server:
  install_method: "helm"
  helm_app_name: "bitnami/metrics-server"
  helm_sets: "rbac.create=true,apiService.create=true"
  url: "https://github.com/kubernetes-incubator/metrics-server"
  plugin_dir: "deploy/1.8+/"

# https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml
# https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
# to patch: kubectl get service kubernetes-dashboard -n kubernetes-dashboard -o json
#
kubernetes_dashboard:
  url: https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml
  namespace: kubernetes-dashboard
  deploy: true
  expose: true
  node_port: 32700

cluster_volumes:
  nfs: true
  local: true

nfs_server:
  nfs_server_addr: 192.168.122.52
  setup: true
  nfs_root_dir: /srv/nfs-volumes
  resticted_network: 192.168.122.0/24
  defaults:
    options: "rw,all_squash,no_subtree_check,insecure"
    size: 5Gi
    accessModes: [ "ReadWriteOnce" ]
    readOnly: false
    persistentVolumeReclaimPolicy: Retain
    storageClassName: slow
    volumeMode: Filesystem
    mountOptions: [ "hard", "nfsvers=4.1" ]
    storageClass: nfs
    namespace: default
  nfs_shares:
    - name: volume001
      options: "rw,no_root_squash,no_subtree_check,insecure"
      size: 10Gi
      accessModes: [ "ReadWriteMany" ]
      claimName: myfirstvolume
    - name: volume002
    - name: volume003
    - name: volume004
      readOnly: true
      persistentVolumeReclaimPolicy: Recycle
    - name: volume005
      options: "rw,no_root_squash,no_subtree_check,insecure"
      size: 10Gi
      accessModes: [ "ReadWriteMany" ]

local_storage:
  defaults:
    size: 5Gi
    accessModes: [ "ReadWriteOnce" ]
    readOnly: false
    persistentVolumeReclaimPolicy: Retain
    storageClassName: local-storage
    volumeMode: Filesystem
    storageClass: local-storage
    root_path: /volumes/k8s_local
    hostsAffinity: [ "node-master02.cluster.local" ]
    namesapce: default
  volumes:
    - name: lvol001
      size: 10Gi
      claimName: myfirstlocalvolume
      storagePath: /volumes/k8s_local/lvol001
    - name: lvol002
      hostsAffinity: [ "node-master02.cluster.local", "node-master01.cluster.local" ]
    - name: lvol003
      hostsAffinity: [ "node-worker01.cluster.local" ]

#
# Advenced Monitoring (Prometheuse, Grafana)
#   echo -n Passw0rd | base64 => UGFzc3cwcmQ=
#   echo -n admin | base64  => YWRtaW4=
#
advanced_monitoring:
  defaults:
    manespace: cluster-monitoring
    use_persistent_volume: false
  prometheus:
    giving_name: prometheus
    expose: true
    host_port: 30000
  grafana:
    giving_name: grafana
    admin_user: "YWRtaW4="
    admin_password: "UGFzc3cwcmQ="
    expose: true
    host_port: 32000

#
# this section should be generated automatically
#
admin_cluster:
  - remove_nodes:
    - node: 192.168.41.132
      reset: true
    - node: 192.168.41.131
      reset: true
  - add_nodes:
    - node: 192.168.41.131
      type: master
    - node: 192.168.41.132
      type: worker
  - options:
      uptade: etc_hosts
      update_inventory: true



