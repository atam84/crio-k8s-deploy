---

reboot_servers_when_cluster_is_destroyed: true
remove_images_when_cluster_is_destroyed: true
generate_hostname_and_create_etc_hosts: false
add_logrotate_system_messages: true
keep_firewalld_enabled: false
upgrade_packages: false
disable_ipv6: true
sshd_use_ipv4_only: true
firewalld_allowed_ports:
  - "6443/tcp"
  - "10250/tcp"
  - "10251/tcp"
  - "10252/tcp"
  - "10255/tcp"
  - "10053/tcp"
  - "10053/udp"
  - "2379/tcp"
  - "2380/tcp"
  - "22/tcp"
  - "80/tcp"
  - "8080/tcp"
  - "443/tcp"
  - "8443/tcp"
  - "9443/tcp"

# Allow any internal communication in the host machine
firewalld_rich_rules:
  - 'rule family="ipv4" source address="0.0.0.0/0" destination address="0.0.0.0/0" accept'
  - 'rule family="ipv4" source address="0.0.0.0/0" destination address="0.0.0.0/0" masquerade'

# List of modules to load br_netfilter and overlay both are important for k8s
modules_to_load:
  - br_netfilter
  - overlay

# If you need to build CNI from source
build_cni_from_source: false
cni_git_url: "https://github.com/containernetworking/plugins"
cni_git_version: "v0.8.1"
cni_build_temp_folder: "~/git"

# CRI-O configuration
crio:
  root: "/var/lib/containers/storage"
  runroot: "/var/run/containers/storage"
  config_directory: "/etc/cni/net.d"
  plugins_directory: '[ "/usr/libexec/cni", "/opt/cni/bin/" ]'
  cgroup_manager: cgroupfs
  storage_driver: overlay
  storage_option: '[ "overlay.override_kernel_check=1" ]'
  stream_addr: "0.0.0.0"
  stream_port: "0"
  file_locking: "false"
  apparmor_profile: "crio-default"
  hooks_dir: '[ "/usr/share/containers/oci/hooks.d" ]'
  log_size_max: 52428800
  log_level: info
  log_dir: "/var/log/crio/pods"

## deprecated will be removed soon
cni_crio_config_directory: "/etc/cni/net.d"
crio_plugins_directory: '[ "/usr/libexec/cni", "/opt/cni/bin/" ]'
crio_cgroup_manager: cgroupfs
## end of deprecation

# Package needed for CNI build
go_build_required_package:
  - golang
  - golang-bin
  - golang-shared
  - golang-tests
  - golang-misc
  - golang-race

# These package should be installed in all node in the preparation stage
package_list:
  - python3
  - logrotate
  - vim
  - jq
  - wget
  - curl
  - git

# () set the master node that performe action or leave black to select it from the masters list using inventory var
master_initiator: 

# Set install true if you need any of these tools in all your node
podman:
  install: true

skopeo:
  install: true

buildah:
  install: true


# What network do you need ? flannel (present issues need to be fixed in the future), weave, calico, canal
pod_network: calico

# You can modify the URL bellow if you want use other version (should be useful in the future)
pod_network_urls:
  flannel: "https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml"
  weave: "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
  calico: "https://docs.projectcalico.org/v3.10/manifests/calico.yaml"
  canal: "https://docs.projectcalico.org/v3.10/manifests/canal.yaml"

# Helm should be installed only in the masters nodes you can add more repos or use your inernal repo and remove the others
# https://github.com/helm/helm/releases
helm:
  install: true
  url: "https://get.helm.sh/helm-v3.0.1-linux-amd64.tar.gz"
  add_repos: true
  update_repos: true
  bin_directory: "/usr/bin"
  repos:
    - name: "incubator"
      url: "https://kubernetes-charts-incubator.storage.googleapis.com/"
    - name: "brigade"
      url: "https://brigadecore.github.io/charts"
    - name: "bitnami"
      url: "https://charts.bitnami.com/bitnami"
    - name: "stable"
      url: "https://kubernetes-charts.storage.googleapis.com"

# metrics_server_install_method accept "git" => {url and plugin_dir become mandatory} or "helm" => {helm_app_name become mandatory}
# in the case of using internal Helm repo you need to modify metrics_server.helm_app_name if you use helm as install method
# otherwise you need to point in your internal git repo for metrics-server if you don't have access to internet
metrics_server:
  install_method: "git"
  helm_app_name: "bitnami/metrics-server"
  helm_sets: "rbac.create=true,apiService.create=true"
  url: "https://github.com/kubernetes-incubator/metrics-server"
  plugin_dir: "deploy/1.8+/"

# https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml
# https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
# to patch: kubectl get service kubernetes-dashboard -n kubernetes-dashboard -o json
# Deploy kubernetes dashboard
kubernetes_dashboard:
  url: https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml
  namespace: kubernetes-dashboard
  deploy: true
  expose: true
  node_port: 32700

# You need to setup some PV and there PVC when the cluster is ployed ? can be useful if you need some persistence quickly
cluster_volumes:
  nfs: false
  local: true

# These informations are used to made your NFS storage and to make the PV and PVC automatically
nfs_server:
  nfs_server_addr: 192.168.122.52
  setup: false
  nfs_root_dir: /srv/nfs-volumes
  resticted_network: 192.168.122.0/24
  defaults:
    options: "rw,all_squash,no_subtree_check,insecure"
    size: 5Gi
    accessModes: [ "ReadWriteOnce" ]
    readOnly: false
    persistentVolumeReclaimPolicy: Retain
    storageClassName: slow
    volumeMode: Filesystem
    mountOptions: [ "hard", "nfsvers=4.1" ]
    storageClass: nfs
    namespace: default
  nfs_shares:
    - name: volume001
      options: "rw,no_root_squash,no_subtree_check,insecure"
      size: 10Gi
      accessModes: [ "ReadWriteMany" ]
      claimName: myfirstvolume
    - name: volume002
    - name: volume003
    - name: volume004
      readOnly: true
      persistentVolumeReclaimPolicy: Recycle
    - name: volume005
      options: "rw,no_root_squash,no_subtree_check,insecure"
      size: 10Gi
      accessModes: [ "ReadWriteMany" ]

# This section is used to configure local storage the same directory structure will be created into nodes
# Important the data is not synchronized between nodes
local_storage:
  defaults:
    size: 5Gi
    accessModes: [ "ReadWriteOnce" ]
    readOnly: false
    persistentVolumeReclaimPolicy: Retain
    storageClassName: local-storage
    volumeMode: Filesystem
    storageClass: local-storage
    root_path: /volumes/k8s_local
    hostsAffinity: [ "node-master02.cluster.local" ]
    namesapce: default
  volumes:
    - name: lvol001
      size: 10Gi
      claimName: myfirstlocalvolume
      storagePath: /volumes/k8s_local/lvol001
    - name: lvol002
      hostsAffinity: [ "node-master02.cluster.local", "node-master01.cluster.local" ]
    - name: lvol003
      hostsAffinity: [ "node-worker01.cluster.local" ]

##
deploy_monitoring_using_helm: true
deploy_monitoring_using_helm_use_pv: false

advanced_monitoring:
  helm: true
  create_rbac: true
  prometheus:
    persistentVolume:
      enable: false
  pushgateway:
    persistentVolume:
      enable: false
  alertmanager:
    persistentVolume:
      enable: false
  grafana:
    persistentVolume:
      enable: false

# If deploy_monitoring_using_helm is true the advenced_monitoring will not be processed
# Advanced Monitoring (Prometheuse, Grafana)
#   echo -n Passw0rd | base64 => UGFzc3cwcmQ=
#   echo -n admin | base64  => YWRtaW4=
advanced_monitoring:
  defaults:
    namespace: cluster-monitoring
    use_persistent_volume: false
    ressources:
      limits:
        cpu: 100m
        memory: 100Mi
      requests:
        cpu: 50m
        memory: 50Mi
  prometheus:
    giving_name: prometheus
    expose: true
    host_port: 30000
  grafana:
    giving_name: grafana
    admin_user: "YWRtaW4="
    admin_password: "UGFzc3cwcmQ="
    expose: true
    host_port: 32000
  kube-state_metrics:
  node-exporter:
  prom-alert-manager:


# https://stackoverflow.com/questions/56959545/prometheus-node-exporter-on-kubernetes
# https://github.com/helm/charts/tree/master/stable/prometheus/templates
# https://github.com/helm/charts/tree/master/stable/prometheus-node-exporter/templates
# https://github.com/do-community/doks-monitoring/tree/master/manifest


#
# this section should be generated automatically in an other file and will be used for the cluster administration (drain node, add node ...)
#
admin_cluster:
  - remove_nodes:
    - node: 192.168.41.132
      reset: true
    - node: 192.168.41.131
      reset: true
  - add_nodes:
    - node: 192.168.41.131
      type: master
    - node: 192.168.41.132
      type: worker
  - options:
      uptade: etc_hosts
      update_inventory: true

